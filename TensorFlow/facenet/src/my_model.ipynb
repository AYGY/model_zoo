{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib\n",
    "import argparse\n",
    "import facenet\n",
    "import lfw\n",
    "import h5py\n",
    "import math\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_encoded_image_string_tensor(encoded_image_string_tensor):\n",
    "    image_tensor = tf.image.decode_image(encoded_image_string_tensor, channels=3)\n",
    "    image_tensor.set_shape((None, None, 3))\n",
    "    return image_tensor\n",
    "\n",
    "def load_model(sess, model, input_map=None):\n",
    "    model_exp = os.path.expanduser(model)\n",
    "    if (os.path.isfile(model_exp)):\n",
    "        print('Model filename: %s' % model_exp)\n",
    "        with gfile.FastGFile(model_exp, 'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, input_map=input_map, name='')\n",
    "    else:\n",
    "        print('Model directory: %s' % model_exp)\n",
    "        meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "        \n",
    "        print('Metagraph file: %s' % meta_file)\n",
    "        print('Checkpoint file: %s' % ckpt_file)\n",
    "        \n",
    "        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n",
    "        saver.restore(sess, os.path.join(model_exp, ckpt_file))\n",
    "\n",
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files)>1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        return meta_file, ckpt_file\n",
    "\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print_tensor_variables(filename):\n",
    "    tensor_variables = tf.global_variables()\n",
    "    with open(filename, 'w') as f:\n",
    "        for variable in tensor_variables:\n",
    "            f.write(str(variable))\n",
    "            f.write('\\n')\n",
    "\n",
    "def debug_print_tensor_trainable_variables(filename):\n",
    "    tensor_variables = tf.trainable_variables()\n",
    "    with open(filename, 'w') as f:\n",
    "        for variable in tensor_variables:\n",
    "            f.write(str(variable))\n",
    "            f.write('\\n')\n",
    "            \n",
    "def debug_print_tensor_operations(filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for op in tf.get_default_graph().get_operations():\n",
    "            f.write(str(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables_and_metagraph(sess, saver, model_dir, model_name):\n",
    "    print('Saving variables')\n",
    "    start_time = time.time()\n",
    "    checkpoint_path = os.path.join(model_dir, 'model-%s.ckpt' % model_name)\n",
    "    saver.save(sess, checkpoint_path, write_meta_graph=False)\n",
    "    save_time_variables = time.time() - start_time\n",
    "    print('Variables saved in %.2f seconds' % save_time_variables)\n",
    "    metagraph_filename = os.path.join(model_dir, 'model-%s.meta' % model_name)\n",
    "    save_time_metagraph = 0  \n",
    "    if not os.path.exists(metagraph_filename):\n",
    "        print('Saving metagraph')\n",
    "        start_time = time.time()\n",
    "        saver.export_meta_graph(metagraph_filename)\n",
    "        save_time_metagraph = time.time() - start_time\n",
    "        print('Metagraph saved in %.2f seconds' % save_time_metagraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/lzhang/facenet/20180408-102900\n",
      "Metagraph file: model-20180408-102900.meta\n",
      "Checkpoint file: model-20180408-102900.ckpt-90\n",
      "INFO:tensorflow:Restoring parameters from /home/lzhang/facenet/20180408-102900/model-20180408-102900.ckpt-90\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "Total 490 copied.\n",
      "Saving variables\n",
      "Variables saved in 0.19 seconds\n",
      "Saving metagraph\n",
      "Metagraph saved in 1.94 seconds\n"
     ]
    }
   ],
   "source": [
    "load_from_model_dir = '/home/lzhang/facenet/20180408-102900'\n",
    "save_to_model_dir = '/home/lzhang/model_zoo/TensorFlow/facenet/models/my'\n",
    "\n",
    "def main():\n",
    "    network = importlib.import_module('models.inception_resnet_v1')\n",
    "    \n",
    "    graph1 = tf.Graph()\n",
    "    with graph1.as_default():\n",
    "        sess1 = tf.Session()\n",
    "        load_model(sess1, load_from_model_dir)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        img_str_placeholder = tf.placeholder(dtype=tf.string, shape=(None, ), name='input')\n",
    "        img_tensor = tf.map_fn(decode_encoded_image_string_tensor, elems=img_str_placeholder, dtype=tf.uint8, back_prop=False)\n",
    "        img_tensor = tf.cast(img_tensor, tf.float32)\n",
    "        img_tensor.set_shape((None, 160, 160, 3))\n",
    "        \n",
    "#         img_tensor = tf.placeholder(dtype=tf.float32, shape=(None, 160, 160, 3), name='input')\n",
    "    \n",
    "        prelogits, _ = network.inference(img_tensor, 1.0, phase_train=False, bottleneck_layer_size=512, weight_decay=0.0)       \n",
    "        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            i = 0\n",
    "            for my_name in tf.trainable_variables():\n",
    "                their_tensor = graph1.get_tensor_by_name(my_name.name)\n",
    "                my_tensor = tf.get_default_graph().get_tensor_by_name(my_name.name)\n",
    "                weight = sess1.run(their_tensor)\n",
    "                sess.run(my_name.assign(weight))\n",
    "                i += 1\n",
    "            print('Total %d copied.' %i)\n",
    "            \n",
    "            save_variables_and_metagraph(sess, saver, save_to_model_dir, 'my')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '/home/lzhang/tmp/0000045'\n",
    "\n",
    "for file in os.listdir(image_dir):\n",
    "    img = Image.open(os.path.join(image_dir, file))\n",
    "    img = img.resize((160, 160))\n",
    "    img.save(os.path.join('/home/lzhang/tmp/0000045_160', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/lzhang/facenet/20180408-102900\n",
      "Metagraph file: model-20180408-102900.meta\n",
      "Checkpoint file: model-20180408-102900.ckpt-90\n",
      "INFO:tensorflow:Restoring parameters from /home/lzhang/facenet/20180408-102900/model-20180408-102900.ckpt-90\n",
      "Model directory: /home/lzhang/model_zoo/TensorFlow/facenet/models/my\n",
      "Metagraph file: model-my.meta\n",
      "Checkpoint file: model-my.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/lzhang/model_zoo/TensorFlow/facenet/models/my/model-my.ckpt\n"
     ]
    }
   ],
   "source": [
    "debug_file = '/home/lzhang/tensorflow_debug.txt'\n",
    "debug_file_my = '/home/lzhang/tensorflow_debug_my.txt'\n",
    "\n",
    "load_from_model_dir = '/home/lzhang/facenet/20180408-102900'\n",
    "save_to_model_dir = '/home/lzhang/model_zoo/TensorFlow/facenet/models/my'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        load_model(sess, load_from_model_dir)\n",
    "#         debug_print_tensor_trainable_variables(debug_file)\n",
    "        debug_print_tensor_operations(debug_file)\n",
    "        \n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        load_model(sess, save_to_model_dir)\n",
    "#         debug_print_tensor_trainable_variables(debug_file_my)\n",
    "        debug_print_tensor_operations(debug_file_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "image_dir = '/home/lzhang/tmp/0000045_160'\n",
    "img_list = []\n",
    "for file in os.listdir(image_dir):\n",
    "    img = Image.open(os.path.join(image_dir, file), 'r')\n",
    "    img = np.array(img)\n",
    "    tf.cast(img, tf.float32)\n",
    "    img = (img - 127.5) / 128.0\n",
    "    img_list.append(img)\n",
    "images = np.stack(img_list)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_people(dataset, people_per_batch, images_per_person):\n",
    "    num_images = people_per_batch * images_per_person\n",
    "    num_classes = len(dataset)\n",
    "    class_index = np.arange(num_classes)\n",
    "    np.random.shuffle(class_index)\n",
    "    \n",
    "    i = 0\n",
    "    image_paths = []\n",
    "    num_per_class = []\n",
    "    sampled_class_index = []\n",
    "    \n",
    "    while len(image_paths) < num_images:\n",
    "        cls_index = class_index[i]\n",
    "        num_images_in_class = len(dataset[cls_index])\n",
    "        image_index = np.arange(num_images_in_class)\n",
    "        np.random.shuffle(image_index)\n",
    "        num_images_from_class = min(num_images_in_class, images_per_person, num_images - len(image_paths))\n",
    "        idx = image_index[:num_images_from_class]\n",
    "        image_path_from_class = [dataset[cls_index].image_paths[j] for j in idx]\n",
    "        sampled_class_index += [cls_index] * num_images_from_class\n",
    "        image_paths += image_path_from_class\n",
    "        num_per_class.append(num_images_from_class)\n",
    "        i += 1\n",
    "    return image_paths, num_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_triplets(embedding, num_images_per_class, image_paths, people_per_batch, alpha):\n",
    "    trip_idx = 0\n",
    "    emb_start_idx = 0\n",
    "    num_trips = 0\n",
    "    triplets = []\n",
    "    \n",
    "    for i in range(people_per_batch):\n",
    "        num_images = int(num_images_per_class[i])\n",
    "        for j in range(1, num_images):\n",
    "            a_idx = emb_start_idx + j - 1\n",
    "            neg_dists_sqr = np.sum(np.square(embedding[a_idx] - embedding), 1)\n",
    "            for pair in range(j, num_images):\n",
    "                p_idx = emb_start_idx + pair\n",
    "                pos_dist_sqr = np.sum(np.square(embedding[a_idx] - embedding[p_idx]))\n",
    "                neg_dists_sqr[emb_start_idx:emb_start_idx + num_images] = np.NaN\n",
    "                all_neg = np.where(np.logical_and(neg_dists_sqr - pos_dist_sqr < alpha, pos_dist_sqr < neg_dists_sqr))[0]\n",
    "                num_random_negs = all_neg.shape[0]\n",
    "                if num_random_negs > 0:\n",
    "                    rnd_idx = np.random.randint(num_random_negs)\n",
    "                    n_idx = all_neg[rnd_idx]\n",
    "                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n",
    "                    trip_idx += 1\n",
    "                num_trips += 1\n",
    "        emb_start_idx += num_images\n",
    "    np.random.shuffle(triplets)\n",
    "    return triplets, num_trips, len(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow3)",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
