{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interpolate\n",
    "from tensorflow.python.training import training\n",
    "import random\n",
    "import re\n",
    "from tensorflow.python.platform import gfile\n",
    "import math\n",
    "from six import iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_arguments_to_file(args, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for key, value in iteritems(vars(args)):\n",
    "            f.write('%s: %s\\n' % (key, str(value)))\n",
    "\n",
    "def store_revision_info(src_path, output_dir, arg_string):\n",
    "    try:\n",
    "        cmd = ['git', 'rev-parse', 'HEAD']\n",
    "        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n",
    "        (stdout, _) = gitproc.communicate()\n",
    "        git_hash = stdout.strip()\n",
    "    except OSError as e:\n",
    "        git_hash = ' '.join(cmd) + ': ' +  e.strerror\n",
    "  \n",
    "    try:\n",
    "        cmd = ['git', 'diff', 'HEAD']\n",
    "        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n",
    "        (stdout, _) = gitproc.communicate()\n",
    "        git_diff = stdout.strip()\n",
    "    except OSError as e:\n",
    "        git_diff = ' '.join(cmd) + ': ' +  e.strerror\n",
    "\n",
    "    rev_info_filename = os.path.join(output_dir, 'revision_info.txt')\n",
    "    with open(rev_info_filename, \"w\") as text_file:\n",
    "        text_file.write('arguments: %s\\n--------------------\\n' % arg_string)\n",
    "        text_file.write('tensorflow version: %s\\n--------------------\\n' % tf.__version__)\n",
    "        text_file.write('git hash: %s\\n--------------------\\n' % git_hash)\n",
    "        text_file.write('%s' % git_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, has_class_directories=True):\n",
    "    dataset = []\n",
    "    path_exp = os.path.expanduser(path)\n",
    "    classes = [path for path in os.listdir(path_exp) if os.path.isdir(os.path.join(path_exp, path))]\n",
    "    classes.sort()\n",
    "    num_classes = len(classes)\n",
    "    for i in range(num_classes):\n",
    "        class_name = classes[i]\n",
    "        face_dir = os.path.join(path_exp, class_name)\n",
    "        image_paths = get_image_paths(face_dir)\n",
    "        dataset.append(ImageClass(class_name, image_paths))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(face_dir):\n",
    "    image_paths = []\n",
    "    if os.path.isdir(face_dir):\n",
    "        images = os.listdir(face_dir)\n",
    "        image_paths = [os.path.join(face_dir, img) for img in images]\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClass():\n",
    "    def __init__(self, name, image_paths):\n",
    "        self.name = name\n",
    "        self.image_paths = image_paths\n",
    "    def __str__(self):\n",
    "        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, split_ratio, min_val_images_per_class, mode):\n",
    "    if mode == 'SPLIT_CLASSES':\n",
    "        num_classes = len(dataset)\n",
    "        class_index = np.arange(num_classes)\n",
    "        np.random.shuffle(class_index)\n",
    "        split = int(round(num_classes * (1 - split_ratio)))\n",
    "        train_set = [dataset[i] for i in class_index[0:split]]\n",
    "        val_set = [dataset[i] for i in class_index[split:-1]]\n",
    "    elif mode == 'SPLIT_IMAGES':\n",
    "        train_set = []\n",
    "        val_set = []\n",
    "        for cls in dataset:\n",
    "            paths = cls.image_paths\n",
    "            np.random.shuffle(paths)\n",
    "            num_images_in_class = len(paths)\n",
    "            split = int(math.floor(num_images_in_class * (1 - split_ratio)))\n",
    "            if split == num_images_in_class:\n",
    "                split = num_images_in_class - 1\n",
    "            if split >= min_val_images_per_class and num_images_in_class - split >= 1:\n",
    "                train_set.append(ImageClass(cls.name, paths[:split]))\n",
    "                val_set.append(ImageClass(cls.name, paths[split:]))\n",
    "    else:\n",
    "        raise ValueError('Invalid train/test split mode \"%s\"' % mode)\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels(dataset):\n",
    "    image_paths_flat = []\n",
    "    labels_flat = []\n",
    "    for i in range(len(dataset)):\n",
    "        image_paths_flat += dataset[i].image_paths\n",
    "        labels_flat += [i] * len(dataset[i].image_paths)\n",
    "    return image_paths_flat, labels_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_pipeline(input_q, image_size, num_preprocess_threads, batch_size_placeholder):\n",
    "    images_and_labels_list = []\n",
    "    for _ in range(num_preprocess_threads):\n",
    "        filenames, label, control = input_q.dequeue()\n",
    "        images = []\n",
    "        for filename in tf.unstack(filenames):\n",
    "            file_contents = tf.read_file(filename)\n",
    "            image = tf.image.decode_image(file_contents, 3)\n",
    "            \n",
    "            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n",
    "                           lambda : tf.random_crop(image, image_size + (3, )), \n",
    "                           lambda : tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n",
    "\n",
    "            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP), \n",
    "                           lambda : tf.image.random_flip_left_right(image), \n",
    "                           lambda : tf.identity(image))\n",
    "\n",
    "            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION), \n",
    "                           lambda : (tf.cast(image, tf.float32) - 127.5) / 128.0, \n",
    "                           lambda : tf.image.per_image_standardization(image))\n",
    "\n",
    "            image = tf.cond(get_control_flag(control[0], FLIP), \n",
    "                           lambda : tf.image.flip_left_right(image), \n",
    "                           lambda : tf.identity(image))\n",
    "            \n",
    "            image.set_shape(image_size + (3, ))\n",
    "            images.append(image)\n",
    "        images_and_labels_list.append([images, label])\n",
    "\n",
    "    image_batch, label_batch = tf.train.batch_join(\n",
    "        images_and_labels_list, batch_size=batch_size_placeholder,\n",
    "        shapes=[image_size + (3, ), ()], enqueue_many=True, \n",
    "        capacity=400 * num_preprocess_threads, allow_smaller_final_batch=True)\n",
    "\n",
    "    return image_batch, label_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_loss(features, label, alpha, num_classes):\n",
    "    num_feature = features.get_shape()[1]\n",
    "    centers = tf.get_variable('centers', [num_classes, num_feature], dtype=tf.float32, \n",
    "                             initializer=tf.constant_initializer(0), trainable=False)\n",
    "    label = tf.reshape(label, [-1])\n",
    "    centers_batch = tf.gather(centers, label)\n",
    "    diff = (1 - alpha) * (centers_batch - features)\n",
    "    centers = tf.scatter_sub(centers, label, diff)\n",
    "    with tf.control_dependencies([centers]):\n",
    "        loss = tf.reduce_mean(tf.square(features - centers_batch))\n",
    "    return loss, centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n",
    "    loss_avg_op = _add_loss_summary(total_loss)\n",
    "    \n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        if optimizer == 'ADAGRAD':\n",
    "            opt = tf.train.AdagradOptimizer(learning_rate)\n",
    "        elif optimizer == 'ADADELTA':\n",
    "            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n",
    "        elif optimizer == 'ADAM':\n",
    "            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n",
    "        elif optimizer == 'RMSPROP':\n",
    "            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n",
    "        elif optimizer == 'MOM':\n",
    "            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimization algorithm')\n",
    "    \n",
    "        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n",
    "        \n",
    "    apply_gradients_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    if log_histograms:\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)    \n",
    "    \n",
    "    var_avg = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)\n",
    "    var_avg_op = var_avg.apply(tf.trainable_variables())\n",
    "    \n",
    "    with tf.control_dependencies([apply_gradients_op, var_avg_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_loss_summary(total_loss):\n",
    "    loss_avg = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_avg_op = loss_avg.apply(losses + [total_loss])\n",
    "    \n",
    "    for loss in losses + [total_loss]:\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', loss)\n",
    "        tf.summary.scalar(l.op.name, loss_avg.average(loss))\n",
    "    return loss_avg_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate_from_file(filename, epoch):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split('#', 1)[0]\n",
    "            if line:\n",
    "                par = line.strip().split(':')\n",
    "                e = int(par[0])\n",
    "                if par[1] == '-':\n",
    "                    lr = -1\n",
    "                else:\n",
    "                    lr = float(par[1])\n",
    "                if e <= epoch:\n",
    "                    learning_rate = lr\n",
    "                else:\n",
    "                    return learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow3)",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
