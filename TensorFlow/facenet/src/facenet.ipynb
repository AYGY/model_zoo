{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interpolate\n",
    "from tensorflow.python.training import training\n",
    "import random\n",
    "import re\n",
    "from tensorflow.python.platform import gfile\n",
    "import math\n",
    "from six import iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_arguments_to_file(args, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for key, value in iteritems(vars(args)):\n",
    "            f.write('%s: %s\\n' % (key, str(value)))\n",
    "\n",
    "def store_revision_info(src_path, output_dir, arg_string):\n",
    "    try:\n",
    "        cmd = ['git', 'rev-parse', 'HEAD']\n",
    "        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n",
    "        (stdout, _) = gitproc.communicate()\n",
    "        git_hash = stdout.strip()\n",
    "    except OSError as e:\n",
    "        git_hash = ' '.join(cmd) + ': ' +  e.strerror\n",
    "  \n",
    "    try:\n",
    "        cmd = ['git', 'diff', 'HEAD']\n",
    "        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n",
    "        (stdout, _) = gitproc.communicate()\n",
    "        git_diff = stdout.strip()\n",
    "    except OSError as e:\n",
    "        git_diff = ' '.join(cmd) + ': ' +  e.strerror\n",
    "\n",
    "    rev_info_filename = os.path.join(output_dir, 'revision_info.txt')\n",
    "    with open(rev_info_filename, \"w\") as text_file:\n",
    "        text_file.write('arguments: %s\\n--------------------\\n' % arg_string)\n",
    "        text_file.write('tensorflow version: %s\\n--------------------\\n' % tf.__version__)\n",
    "        text_file.write('git hash: %s\\n--------------------\\n' % git_hash)\n",
    "        text_file.write('%s' % git_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, has_class_directories=True):\n",
    "    dataset = []\n",
    "    path_exp = os.path.expanduser(path)\n",
    "    classes = [path for path in os.listdir(path_exp) if os.path.isdir(os.path.join(path_exp, path))]\n",
    "    classes.sort()\n",
    "    num_classes = len(classes)\n",
    "    for i in range(num_classes):\n",
    "        class_name = classes[i]\n",
    "        face_dir = os.path.join(path_exp, class_name)\n",
    "        image_paths = get_image_paths(face_dir)\n",
    "        dataset.append(ImageClass(class_name, image_paths))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(face_dir):\n",
    "    image_paths = []\n",
    "    if os.path.isdir(face_dir):\n",
    "        images = os.listdir(face_dir)\n",
    "        image_paths = [os.path.join(face_dir, img) for img in images]\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClass():\n",
    "    def __init__(self, name, image_paths):\n",
    "        self.name = name\n",
    "        self.image_paths = image_paths\n",
    "    def __str__(self):\n",
    "        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, split_ratio, min_val_images_per_class, mode):\n",
    "    if mode == 'SPLIT_CLASSES':\n",
    "        num_classes = len(dataset)\n",
    "        class_index = np.arange(num_classes)\n",
    "        np.random.shuffle(class_index)\n",
    "        split = int(round(num_classes * (1 - split_ratio)))\n",
    "        train_set = [dataset[i] for i in class_index[0:split]]\n",
    "        val_set = [dataset[i] for i in class_index[split:-1]]\n",
    "    elif mode == 'SPLIT_IMAGES':\n",
    "        train_set = []\n",
    "        val_set = []\n",
    "        for cls in dataset:\n",
    "            paths = cls.image_paths\n",
    "            np.random.shuffle(paths)\n",
    "            num_images_in_class = len(paths)\n",
    "            split = int(math.floor(num_images_in_class * (1 - split_ratio)))\n",
    "            if split == num_images_in_class:\n",
    "                split = num_images_in_class - 1\n",
    "            if split >= min_val_images_per_class and num_images_in_class - split >= 1:\n",
    "                train_set.append(ImageClass(cls.name, paths[:split]))\n",
    "                val_set.append(ImageClass(cls.name, paths[split:]))\n",
    "    else:\n",
    "        raise ValueError('Invalid train/test split mode \"%s\"' % mode)\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels(dataset):\n",
    "    image_paths_flat = []\n",
    "    labels_flat = []\n",
    "    for i in range(len(dataset)):\n",
    "        image_paths_flat += dataset[i].image_paths\n",
    "        labels_flat += [i] * len(dataset[i].image_paths)\n",
    "    return image_paths_flat, labels_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_pipeline(input_q, image_size, num_preprocess_threads, batch_size_placeholder):\n",
    "    images_and_labels_list = []\n",
    "    for _ in range(num_preprocess_threads):\n",
    "        filenames, label, control = input_q.dequeue()\n",
    "        images = []\n",
    "        for filename in tf.unstack(filenames):\n",
    "            file_contents = tf.read_file(filename)\n",
    "            image = tf.image.decode_image(file_contents, 3)\n",
    "            \n",
    "            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n",
    "                           lambda : tf.random_crop(image, image_size + (3, )), \n",
    "                           lambda : tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n",
    "\n",
    "            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP), \n",
    "                           lambda : tf.image.random_flip_left_right(image), \n",
    "                           lambda : tf.identity(image))\n",
    "\n",
    "            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION), \n",
    "                           lambda : (tf.cast(image, tf.float32) - 127.5) / 128.0, \n",
    "                           lambda : tf.image.per_image_standardization(image))\n",
    "\n",
    "            image = tf.cond(get_control_flag(control[0], FLIP), \n",
    "                           lambda : tf.image.flip_left_right(image), \n",
    "                           lambda : tf.identity(image))\n",
    "            \n",
    "            image.set_shape(image_size + (3, ))\n",
    "            images.append(image)\n",
    "        images_and_labels_list.append([images, label])\n",
    "\n",
    "    image_batch, label_batch = tf.train.batch_join(\n",
    "        images_and_labels_list, batch_size=batch_size_placeholder,\n",
    "        shapes=[image_size + (3, ), ()], enqueue_many=True, \n",
    "        capacity=400 * num_preprocess_threads, allow_smaller_final_batch=True)\n",
    "\n",
    "    return image_batch, label_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_loss(features, label, alpha, num_classes):\n",
    "    num_feature = features.get_shape()[1]\n",
    "    centers = tf.get_variable('centers', [num_classes, num_feature], dtype=tf.float32, \n",
    "                             initializer=tf.constant_initializer(0), trainable=False)\n",
    "    label = tf.reshape(label, [-1])\n",
    "    centers_batch = tf.gather(centers, label)\n",
    "    diff = (1 - alpha) * (centers_batch - features)\n",
    "    centers = tf.scatter_sub(centers, label, diff)\n",
    "    with tf.control_dependencies([centers]):\n",
    "        loss = tf.reduce_mean(tf.square(features - centers_batch))\n",
    "    return loss, centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n",
    "    loss_avg_op = _add_loss_summary(total_loss)\n",
    "    \n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        if optimizer == 'ADAGRAD':\n",
    "            opt = tf.train.AdagradOptimizer(learning_rate)\n",
    "        elif optimizer == 'ADADELTA':\n",
    "            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n",
    "        elif optimizer == 'ADAM':\n",
    "            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n",
    "        elif optimizer == 'RMSPROP':\n",
    "            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n",
    "        elif optimizer == 'MOM':\n",
    "            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimization algorithm')\n",
    "    \n",
    "        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n",
    "        \n",
    "    apply_gradients_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    if log_histograms:\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)    \n",
    "    \n",
    "    var_avg = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)\n",
    "    var_avg_op = var_avg.apply(tf.trainable_variables())\n",
    "    \n",
    "    with tf.control_dependencies([apply_gradients_op, var_avg_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_loss_summary(total_loss):\n",
    "    loss_avg = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_avg_op = loss_avg.apply(losses + [total_loss])\n",
    "    \n",
    "    for loss in losses + [total_loss]:\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', loss)\n",
    "        tf.summary.scalar(l.op.name, loss_avg.average(loss))\n",
    "    return loss_avg_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate_from_file(filename, epoch):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split('#', 1)[0]\n",
    "            if line:\n",
    "                par = line.strip().split(':')\n",
    "                e = int(par[0])\n",
    "                if par[1] == '-':\n",
    "                    lr = -1\n",
    "                else:\n",
    "                    lr = float(par[1])\n",
    "                if e <= epoch:\n",
    "                    learning_rate = lr\n",
    "                else:\n",
    "                    return learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, input_map=None):\n",
    "    model_exp = os.path.expanduser(model)\n",
    "    if (os.path.isfile(model_exp)):\n",
    "        print('Model filename: %s' % model_exp)\n",
    "        with gfile.FastGFile(model_exp, 'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, input_map=input_map, name='')\n",
    "    else:\n",
    "        print('Model directory: %s' % model_exp)\n",
    "        meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "        \n",
    "        print('Metagraph file: %s' % meta_file)\n",
    "        print('Checkpoint file: %s' % ckpt_file)\n",
    "        \n",
    "        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n",
    "        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files)>1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        return meta_file, ckpt_file\n",
    "\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/lzhang/facenet/20180408-102900\n",
      "Metagraph file: model-20180408-102900.meta\n",
      "Checkpoint file: model-20180408-102900.ckpt-90\n",
      "INFO:tensorflow:Restoring parameters from /home/lzhang/facenet/20180408-102900/model-20180408-102900.ckpt-90\n",
      "WARNING:tensorflow:From <ipython-input-34-536dd319a06f>:16: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    }
   ],
   "source": [
    "def debug_print_tensor_variables():\n",
    "    tensor_variables = tf.global_variables()\n",
    "    with open('/home/lzhang/tensorflow_vars.txt', 'w') as f:\n",
    "        for variable in tensor_variables:\n",
    "            f.write(str(variable))\n",
    "            f.write('\\n')\n",
    "\n",
    "def debug_print_tensor_trainable_variables():\n",
    "    tensor_variables = tf.trainable_variables()\n",
    "    with open('/home/lzhang/tensorflow_trainable_vars.txt', 'w') as f:\n",
    "        for variable in tensor_variables:\n",
    "            f.write(str(variable))\n",
    "            f.write('\\n')\n",
    "            \n",
    "def debug_print_tensor_operations():\n",
    "    with open('/home/lzhang/tensorflow_ops.txt', 'w') as f:\n",
    "        for op in tf.get_default_graph().get_operations():\n",
    "            f.write(str(op))\n",
    "            \n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        load_model('/home/lzhang/facenet/20180408-102900')\n",
    "        debug_print_tensor_variables()\n",
    "        debug_print_tensor_operations()\n",
    "        debug_print_tensor_trainable_variables()\n",
    "        debug_print_tensor_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "image_dir = '/home/lzhang/tmp/0000045_160'\n",
    "\n",
    "def facenet_client():\n",
    "    img_list = []\n",
    "    for file in os.listdir(image_dir):\n",
    "        img = Image.open(os.path.join(image_dir, file))\n",
    "#         img = img.resize((160, 160))\n",
    "        img = np.array(img)\n",
    "        tf.cast(img, tf.float32)\n",
    "#         img = np.expand_dims(img, 0)\n",
    "        img_list.append(img)\n",
    "    images = np.stack(img_list)\n",
    "    print(images.shape)\n",
    "    \n",
    "facenet_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow3)",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
